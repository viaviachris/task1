{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3971,"databundleVersionId":32703,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade transformers huggingface-hub requests","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\n\n# åŠ è½½æ¨¡å‹ï¼ˆæ³¨æ„ï¼šdeberta-v2-xxlarge ä½“ç§¯å¾ˆå¤§ï¼Œéœ€è¦è€å¿ƒç­‰å¾…ä¸‹è½½ï¼‰\npipe = pipeline(\n    \"fill-mask\",\n    model=\"microsoft/deberta-v2-xxlarge\",\n    framework=\"pt\",  # å¼ºåˆ¶ä½¿ç”¨ PyTorch æ¡†æ¶\n    trust_remote_code=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"deberata_prompt","metadata":{}},{"cell_type":"code","source":"# Use a pipeline as a high-level helper\nfrom transformers import pipeline\npipe = pipeline(\"fill-mask\", model=\"microsoft/deberta-v2-xxlarge\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoModel\nmodel = AutoModel.from_pretrained(\"microsoft/deberta-v2-xxlarge\", torch_dtype=\"auto\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"deberta_prompt","metadata":{}},{"cell_type":"code","source":"# import os\n# import sys\n# import logging\n# import datasets\n# import evaluate\n\n# import pandas as pd\n# import numpy as np\n\n# from transformers import AutoModelForSequenceClassification, DebertaV2Tokenizer, DataCollatorWithPadding\n# from transformers import Trainer, TrainingArguments\n# from peft import PromptTuningConfig, get_peft_model, TaskType\n# from sklearn.model_selection import train_test_split\n\n# train = pd.read_csv(\"/kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\", header=0, delimiter=\"\\t\", quoting=3)\n# test = pd.read_csv(\"/kaggle/input/word2vec-nlp-tutorial/testData.tsv.zip\", header=0, delimiter=\"\\t\", quoting=3)\n\n# if __name__ == '__main__':\n#     program = os.path.basename(sys.argv[0])\n#     logger = logging.getLogger(program)\n\n#     logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n#     logging.root.setLevel(level=logging.INFO)\n#     logger.info(r\"running %s\" % ''.join(sys.argv))\n\n#     train, val = train_test_split(train, test_size=.2)\n\n#     train_dict = {'label': train[\"sentiment\"], 'text': train['review']}\n#     val_dict = {'label': val[\"sentiment\"], 'text': val['review']}\n#     test_dict = {\"text\": test['review']}\n\n#     train_dataset = datasets.Dataset.from_dict(train_dict)\n#     val_dataset = datasets.Dataset.from_dict(val_dict)\n#     test_dataset = datasets.Dataset.from_dict(test_dict)\n\n#     batch_size = 16\n\n#     model_id = \"microsoft/deberta-v2-xxlarge\"\n\n#     tokenizer = DebertaV2Tokenizer.from_pretrained(model_id)\n\n\n#     def preprocess_function(examples):\n#         return tokenizer(examples['text'], truncation=True, max_length=256)\n\n\n#     tokenized_train = train_dataset.map(preprocess_function, batched=True)\n#     tokenized_val = val_dataset.map(preprocess_function, batched=True)\n#     tokenized_test = test_dataset.map(preprocess_function, batched=True)\n\n#     data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n#     model = AutoModelForSequenceClassification.from_pretrained(model_id)\n\n#     # Define LoRA Config\n#     peft_config = PromptTuningConfig(\n#         num_virtual_tokens=10,\n#         task_type=TaskType.SEQ_CLS\n#     )\n\n#     # prepare int-8 model for training\n#     # model = prepare_model_for_int8_training(model)\n\n#     # add LoRA adaptor\n#     model = get_peft_model(model, peft_config)\n#     model.print_trainable_parameters()\n\n#     metric = evaluate.load(\"accuracy\")\n\n\n#     def compute_metrics(eval_pred):\n#         logits, labels = eval_pred\n#         predictions = np.argmax(logits, axis=-1)\n#         return metric.compute(predictions=predictions, references=labels)\n\n\n#     training_args = TrainingArguments(\n#         output_dir='./checkpoint',  # output directory\n#         num_train_epochs=3,  # total number of training epochs\n#         per_device_train_batch_size=2,  # batch size per device during training\n#         per_device_eval_batch_size=4,  # batch size for evaluation\n#         warmup_steps=500,  # number of warmup steps for learning rate scheduler\n#         weight_decay=0.01,  # strength of weight decay\n#         logging_dir='./logs',  # directory for storing logs\n#         logging_steps=100,\n#         save_strategy=\"no\",\n#         eval_strategy=\"epoch\",\n#         report_to=\"none\"  # ç¦ç”¨wandb\n        \n#     )\n \n#     trainer = Trainer(\n#         model=model,  # the instantiated ğŸ¤— Transformers model to be trained\n#         args=training_args,  # training arguments, defined above\n#         train_dataset=tokenized_train,  # training dataset\n#         eval_dataset=tokenized_val,  # evaluation dataset\n#         tokenizer=tokenizer,\n#         data_collator=data_collator,\n#         compute_metrics=compute_metrics,\n#     )\n\n#     trainer.train()\n\n#     prediction_outputs = trainer.predict(tokenized_test)\n#     test_pred = np.argmax(prediction_outputs[0], axis=-1).flatten()\n#     print(test_pred)\n\n#     result_output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": test_pred})\n#     result_output.to_csv(\"/kaggle/working/deberta_lora_int8.csv\", index=False, quoting=3)\n#     logging.info('result saved!')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"deberata_lora","metadata":{}},{"cell_type":"code","source":"# import os\n# import sys\n# import logging\n# import datasets\n# import evaluate\n\n# import pandas as pd\n# import numpy as np\n\n# from transformers import AutoModelForSequenceClassification, DebertaV2Tokenizer, DataCollatorWithPadding\n# from transformers import Trainer, TrainingArguments\n# from peft import LoraConfig, get_peft_model,TaskType\n# from peft.utils import prepare_model_for_kbit_training  # æ›¿æ¢åŸæ¥çš„ prepare_model_for_int8_training\n# from sklearn.model_selection import train_test_split\n\n# train = pd.read_csv(\"/kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\", header=0, delimiter=\"\\t\", quoting=3)\n# test = pd.read_csv(\"/kaggle/input/word2vec-nlp-tutorial/testData.tsv.zip\", header=0, delimiter=\"\\t\", quoting=3)\n\n# if __name__ == '__main__':\n#     program = os.path.basename(sys.argv[0])\n#     logger = logging.getLogger(program)\n\n#     logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n#     logging.root.setLevel(level=logging.INFO)\n#     logger.info(r\"running %s\" % ''.join(sys.argv))\n\n#     train, val = train_test_split(train, test_size=.2)\n\n#     train_dict = {'label': train[\"sentiment\"], 'text': train['review']}\n#     val_dict = {'label': val[\"sentiment\"], 'text': val['review']}\n#     test_dict = {\"text\": test['review']}\n\n#     train_dataset = datasets.Dataset.from_dict(train_dict)\n#     val_dataset = datasets.Dataset.from_dict(val_dict)\n#     test_dataset = datasets.Dataset.from_dict(test_dict)\n\n#     batch_size = 16\n\n#     model_id = \"microsoft/deberta-v2-xxlarge\"\n\n#     tokenizer = DebertaV2Tokenizer.from_pretrained(model_id)\n\n\n#     def preprocess_function(examples):\n#         return tokenizer(examples['text'], truncation=True, max_length=256)\n\n\n#     tokenized_train = train_dataset.map(preprocess_function, batched=True)\n#     tokenized_val = val_dataset.map(preprocess_function, batched=True)\n#     tokenized_test = test_dataset.map(preprocess_function, batched=True)\n\n#     data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n#     model = AutoModelForSequenceClassification.from_pretrained(\n#         model_id,\n#         # device_map=\"auto\",\n#         # load_in_8bit=True\n#     )\n\n#     # Define LoRA Config\n#     lora_config = LoraConfig(\n#         r=16,\n#         lora_alpha=32,\n#         # target_modules=['q_proj', 'v_proj'],\n#         lora_dropout=0.05,\n#         bias=\"none\",\n#         task_type=TaskType.SEQ_CLS\n#     )\n\n#     # prepare int-8 model for training\n#     # model = prepare_model_for_int8_training(model)\n\n#     # add LoRA adaptor\n#     model = get_peft_model(model, lora_config)\n#     model.print_trainable_parameters()\n\n#     metric = evaluate.load(\"accuracy\")\n\n\n#     def compute_metrics(eval_pred):\n#         logits, labels = eval_pred\n#         predictions = np.argmax(logits, axis=-1)\n#         return metric.compute(predictions=predictions, references=labels)\n\n\n#     training_args = TrainingArguments(\n#         output_dir='./checkpoint',  # output directory\n#         num_train_epochs=3,  # total number of training epochs\n#         per_device_train_batch_size=2,  # batch size per device during training\n#         per_device_eval_batch_size=4,  # batch size for evaluation\n#         warmup_steps=500,  # number of warmup steps for learning rate scheduler\n#         weight_decay=0.01,  # strength of weight decay\n#         logging_dir='./logs',  # directory for storing logs\n#         logging_steps=100,\n#         save_strategy=\"no\",\n#         eval_strategy=\"epoch\",\n#         report_to=\"none\"  # ç¦ç”¨wandb\n#     )\n\n#     trainer = Trainer(\n#         model=model,  # the instantiated ğŸ¤— Transformers model to be trained\n#         args=training_args,  # training arguments, defined above\n#         train_dataset=tokenized_train,  # training dataset\n#         eval_dataset=tokenized_val,  # evaluation dataset\n#         tokenizer=tokenizer,\n#         data_collator=data_collator,\n#         compute_metrics=compute_metrics,\n#     )\n\n#     trainer.train()\n\n#     prediction_outputs = trainer.predict(tokenized_test)\n#     test_pred = np.argmax(prediction_outputs[0], axis=-1).flatten()\n#     print(test_pred)\n\n#     result_output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": test_pred})\n#     result_output.to_csv(\"/kaggle/working/deberta_lora_int8.csv\", index=False, quoting=3)\n#     logging.info('result saved!')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"deberta_prefix","metadata":{}},{"cell_type":"code","source":"!pip install peft -q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade \"transformers>=4.38\" \"peft>=0.8\" \"accelerate>=0.27\" \"datasets>=2.14\" \"evaluate\" -q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============ ç¯å¢ƒå‡†å¤‡ ============\n!pip install --upgrade transformers>=4.38 peft>=0.8 accelerate>=0.27 datasets evaluate -q\n\nimport os\nimport sys\nimport logging\nimport pandas as pd\nimport numpy as np\n\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\n\nfrom transformers import (\n    BartTokenizer,\n    BartForConditionalGeneration,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    EarlyStoppingCallback\n)\nfrom peft import PrefixTuningConfig, get_peft_model, TaskType\nimport evaluate\n\n# ============ åŠ è½½æ•°æ® ============\ntrain_df = pd.read_csv(\"/kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\", \n                       header=0, delimiter=\"\\t\", quoting=3)\ntest_df = pd.read_csv(\"/kaggle/input/word2vec-nlp-tutorial/testData.tsv.zip\", \n                      header=0, delimiter=\"\\t\", quoting=3)\n\n# è½¬æ¢æ ‡ç­¾ä¸ºæ–‡æœ¬\nlabel_map = {1: \"positive\", 0: \"negative\"}\ntrain_df[\"target\"] = train_df[\"sentiment\"].map(label_map)\ntest_df[\"target\"] = \"\"  # æµ‹è¯•é›†æ— æ ‡ç­¾\n\n# åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†\ntrain, val = train_test_split(train_df, test_size=0.2, random_state=42)\n\n# æ„å»ºè¾“å…¥è¾“å‡ºæ ¼å¼\ndef format_instruction(review):\n    return f\"Classify the sentiment of this movie review: {review}\"\n\ntrain_dataset = Dataset.from_pandas(train[[\"review\", \"target\"]])\nval_dataset = Dataset.from_pandas(val[[\"review\", \"target\"]])\ntest_dataset = Dataset.from_pandas(test_df[[\"review\", \"target\"]])\n\n# ============ æ¨¡å‹å’Œ tokenizer ============\nmodel_id = \"facebook/bart-large\"\ntokenizer = BartTokenizer.from_pretrained(model_id)\nmodel = BartForConditionalGeneration.from_pretrained(model_id)\n\n# æ·»åŠ ç‰¹æ®Š tokenï¼ˆå¯é€‰ï¼Œä½†æ¨èï¼‰\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\nmodel.resize_token_embeddings(len(tokenizer))\n\n# ============ æ•°æ®é¢„å¤„ç† ============\nmax_input_length = 512\nmax_target_length = 8\n\ndef preprocess_function(examples):\n    inputs = [format_instruction(review) for review in examples[\"review\"]]\n    targets = examples[\"target\"]\n    \n    model_inputs = tokenizer(\n        inputs,\n        max_length=max_input_length,\n        truncation=True,\n        padding=False\n    )\n    \n    # Tokenize targets\n    labels = tokenizer(\n        targets,\n        max_length=max_target_length,\n        truncation=True,\n        padding=False\n    )\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized_train = train_dataset.map(preprocess_function, batched=True)\ntokenized_val = val_dataset.map(preprocess_function, batched=True)\ntokenized_test = test_dataset.map(preprocess_function, batched=True)\n\n# ============ PEFT: Prefix Tuning ============\npeft_config = PrefixTuningConfig(\n    task_type=TaskType.SEQ_2_SEQ_LM,      # æ³¨æ„ï¼šè¿™é‡Œæ˜¯ SEQ_2_SEQ_LM\n    num_virtual_tokens=20,\n    encoder_hidden_size=1024              # bart-large hidden size\n)\n\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\n\n# ============ æ•°æ®æ•´ç†å™¨ ============\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n\n# ============ è¯„ä¼°æŒ‡æ ‡ ============\naccuracy = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    preds, labels = eval_pred\n    \n    # è§£ç é¢„æµ‹\n    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    \n    # è§£ç æ ‡ç­¾\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # è½¬å›æ•°å­—æ ‡ç­¾\n    pred_labels = [1 if p.strip().lower() == \"positive\" else 0 for p in decoded_preds]\n    true_labels = [1 if l.strip().lower() == \"positive\" else 0 for l in decoded_labels]\n    \n    return accuracy.compute(predictions=pred_labels, references=true_labels)\n\n# ============ è®­ç»ƒå‚æ•° ============\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./bart_seq2seq_prefix\",\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=4,  # å‡å°‘æ˜¾å­˜å‹åŠ›\n    warmup_steps=200,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=50,\n    eval_strategy=\"epoch\",\n    save_strategy=\"no\",\n    predict_with_generate=True,   # å¿…é¡»å¼€å¯æ‰èƒ½ç”Ÿæˆæ–‡æœ¬ç”¨äºè¯„ä¼°\n    generation_max_length=8,\n    generation_num_beams=1,       # greedy decoding\n    load_best_model_at_end=False,\n    report_to=\"none\",\n    fp16=True,                    # å¯ç”¨æ··åˆç²¾åº¦åŠ é€Ÿ\n)\n\n# ============ Trainer ============\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\n# ============ å¼€å§‹è®­ç»ƒ ============\ntrainer.train()\n\n# ============ é¢„æµ‹æµ‹è¯•é›† ============\npredictions = trainer.predict(tokenized_test)\npred_ids = predictions.predictions\n\n# å¤„ç†ç”Ÿæˆç»“æœ\npred_ids = np.where(pred_ids != -100, pred_ids, tokenizer.pad_token_id)\ndecoded_preds = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n\n# è½¬ä¸º 0/1 æ ‡ç­¾\ntest_pred = []\nfor p in decoded_preds:\n    p_clean = p.strip().lower()\n    if \"positive\" in p_clean:\n        test_pred.append(1)\n    elif \"negative\" in p_clean:\n        test_pred.append(0)\n    else:\n        # fallback: çœ‹æ˜¯å¦æ›´æ¥è¿‘å“ªä¸ªè¯\n        test_pred.append(1 if \"pos\" in p_clean else 0)\n\n# ============ ä¿å­˜ç»“æœ ============\nresult_output = pd.DataFrame({\n    \"id\": test_df[\"id\"],\n    \"sentiment\": test_pred\n})\nresult_output.to_csv(\"/kaggle/working/bart_seq2seq_prefix.csv\", index=False, quoting=3)\nlogging.info('âœ… Result saved!')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"deberta_putuning","metadata":{}},{"cell_type":"code","source":"# import os\n# import sys\n# import logging\n# import datasets\n# import evaluate\n\n# import pandas as pd\n# import numpy as np\n\n# from transformers import AutoModelForSequenceClassification, DebertaV2Tokenizer, DataCollatorWithPadding\n# from transformers import Trainer, TrainingArguments\n# from peft import PromptEncoderConfig, get_peft_model, TaskType\n# from peft.utils import prepare_model_for_kbit_training  # æ›¿æ¢åŸæ¥çš„ prepare_model_for_int8_training\n# from sklearn.model_selection import train_test_split\n\n# train = pd.read_csv(\"/kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\", header=0, delimiter=\"\\t\", quoting=3)\n# test = pd.read_csv(\"/kaggle/input/word2vec-nlp-tutorial/testData.tsv.zip\", header=0, delimiter=\"\\t\", quoting=3)\n\n# if __name__ == '__main__':\n#     program = os.path.basename(sys.argv[0])\n#     logger = logging.getLogger(program)\n\n#     logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n#     logging.root.setLevel(level=logging.INFO)\n#     logger.info(r\"running %s\" % ''.join(sys.argv))\n\n#     train, val = train_test_split(train, test_size=.2)\n\n#     train_dict = {'label': train[\"sentiment\"], 'text': train['review']}\n#     val_dict = {'label': val[\"sentiment\"], 'text': val['review']}\n#     test_dict = {\"text\": test['review']}\n\n#     train_dataset = datasets.Dataset.from_dict(train_dict)\n#     val_dataset = datasets.Dataset.from_dict(val_dict)\n#     test_dataset = datasets.Dataset.from_dict(test_dict)\n\n#     batch_size = 16\n\n#     model_id = \"microsoft/deberta-v2-xxlarge\"\n\n#     tokenizer = DebertaV2Tokenizer.from_pretrained(model_id)\n\n\n#     def preprocess_function(examples):\n#         return tokenizer(examples['text'], truncation=True, max_length=256)\n\n\n#     tokenized_train = train_dataset.map(preprocess_function, batched=True)\n#     tokenized_val = val_dataset.map(preprocess_function, batched=True)\n#     tokenized_test = test_dataset.map(preprocess_function, batched=True)\n\n#     data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n#     model = AutoModelForSequenceClassification.from_pretrained(model_id)\n\n#     # Define LoRA Config\n#     peft_config = PromptEncoderConfig(\n#         num_virtual_tokens=20,\n#         encoder_hidden_size=128,\n#         task_type=TaskType.SEQ_CLS\n#     )\n\n#     # prepare int-8 model for training\n#     # model = prepare_model_for_int8_training(model)\n\n#     # add LoRA adaptor\n#     model = get_peft_model(model, peft_config)\n#     model.print_trainable_parameters()\n\n#     metric = evaluate.load(\"accuracy\")\n\n\n#     def compute_metrics(eval_pred):\n#         logits, labels = eval_pred\n#         predictions = np.argmax(logits, axis=-1)\n#         return metric.compute(predictions=predictions, references=labels)\n\n\n#     training_args = TrainingArguments(\n#         output_dir='./checkpoint',  # output directory\n#         num_train_epochs=3,  # total number of training epochs\n#         per_device_train_batch_size=2,  # batch size per device during training\n#         per_device_eval_batch_size=4,  # batch size for evaluation\n#         warmup_steps=500,  # number of warmup steps for learning rate scheduler\n#         weight_decay=0.01,  # strength of weight decay\n#         logging_dir='./logs',  # directory for storing logs\n#         logging_steps=100,\n#         save_strategy=\"no\",\n#         eval_strategy=\"epoch\",\n#         report_to=\"none\"  # ç¦ç”¨wandb\n        \n#     )\n\n#     trainer = Trainer(\n#         model=model,  # the instantiated ğŸ¤— Transformers model to be trained\n#         args=training_args,  # training arguments, defined above\n#         train_dataset=tokenized_train,  # training dataset\n#         eval_dataset=tokenized_val,  # evaluation dataset\n#         tokenizer=tokenizer,\n#         data_collator=data_collator,\n#         compute_metrics=compute_metrics,\n#     )\n\n#     trainer.train()\n\n#     prediction_outputs = trainer.predict(tokenized_test)\n#     test_pred = np.argmax(prediction_outputs[0], axis=-1).flatten()\n#     print(test_pred)\n\n#     result_output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": test_pred})\n#     result_output.to_csv(\"/kaggle/working/deberta_putuning.csv\", index=False, quoting=3)\n#     logging.info('result saved!')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}